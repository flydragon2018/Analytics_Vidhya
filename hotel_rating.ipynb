{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hotel_rating.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0HVRZ+qbao7g38GSqz3Ro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flydragon2018/Analytics_Vidhya/blob/master/hotel_rating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ_lS5T4eur9",
        "outputId": "530014e4-8053-4474-f90a-468df9ad0fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #, force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFjsQ-Q2e8wv",
        "outputId": "d5b72b8d-a4b3-498c-e1d5-3e33b34b8175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!unzip -q /content/drive/\"My Drive\"/hotel/Hotel_rating.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imQbyIbJhBlQ",
        "outputId": "36861109-4ff4-4ee3-e2dc-2fd38746eff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/Shemka/ReviewsScoring.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ReviewsScoring' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqG0a-PLh16g",
        "outputId": "9154045c-3a68-427a-ef5f-6e44fa381b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKnApC9vi-bu"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiWPBViOi-fk"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd7a73JxiApr"
      },
      "source": [
        "!cat  /content/ReviewsScoring/requirements.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHBGNu9FhKZM"
      },
      "source": [
        "!sed 's/en-core-web-sm==2.1.0/en-core-web-sm/' /content/ReviewsScoring/requirements.txt > /content/ReviewsScoring/requirements.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzSlkyBHjPYj"
      },
      "source": [
        "!pip install -r /content/ReviewsScoring/requirements.txt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdL74w8NjPsT"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCqST1lre84B"
      },
      "source": [
        "#!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tybrMQ7sjnlR"
      },
      "source": [
        "#!cp -rf /content/ReviewsScoring/* ."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0xrc6WyjnpU"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3KwJSGe87f"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOT13RDge8-o",
        "outputId": "1f58e73a-ebaa-4d23-a74a-af8bad92a2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# MAIN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import gc\n",
        "import pickle\n",
        "import joblib\n",
        "from scripts.storing import save_model, load_model\n",
        "from scripts.cleanup import CleanUpText\n",
        "\n",
        "# NLP\n",
        "import spacy\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# SKLEARN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_squared_error, r2_score\n",
        "\n",
        "# TENSORFLOW\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Embedding, Reshape, Flatten, Conv1D, SpatialDropout1D, MaxPooling1D, Dense, GRU, LSTM, Dropout, BatchNormalization, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccwm1EBqj0Bj",
        "outputId": "6252c296-5edd-4da6-cbbc-9ea87faa1b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "reviews = pd.read_csv('train.csv') \n",
        "#reviews = reviews[['Reviewer_Score', 'review']]\n",
        "reviews.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good place stay check rainforest biobay vieque...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>great firstly did n't enjoy hong kong, 3 days ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>clean convenient hotel catedral ideally locate...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transport good class high excellent communicat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>stay happy la quinta, used stay travelodge str...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  rating\n",
              "0  good place stay check rainforest biobay vieque...       4\n",
              "1  great firstly did n't enjoy hong kong, 3 days ...       5\n",
              "2  clean convenient hotel catedral ideally locate...       4\n",
              "3  transport good class high excellent communicat...       3\n",
              "4  stay happy la quinta, used stay travelodge str...       4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OihVAq8j0E6",
        "outputId": "2c58e194-7d01-4a72-f288-aa792779f147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = CleanUpText().fit_transform(reviews['review'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16392/16392 [00:21<00:00, 746.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqA6sFz7j0IO",
        "outputId": "f5b3a746-46ad-4354-87ad-439f85c0b0d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Data splitting and tokenizing \n",
        "X_train, X_val, y_train, y_val = train_test_split(text, reviews['rating']/5, random_state=99, test_size=0.1)\n",
        "# Vocabulary len\n",
        "NUM_WORDS = 50000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35461 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOSq8Vc0j0LB",
        "outputId": "7d165487-8c44-405f-f3ed-8b9bf2e94649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# Save tokenizer object\n",
        "save_model(tokenizer, 'my_tokenizer.pkl')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model was saved in my_tokenizer.pkl\n",
            "Use load_model to load model from file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSqQt4Lj0OA",
        "outputId": "1ac3de08-66ab-4796-b2a1-7b9de8644ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%%time\n",
        "# Sequence cropping\n",
        "X_train = pad_sequences(X_train)\n",
        "X_val = pad_sequences(X_val, maxlen=X_train.shape[1])\n",
        "print('Shape of X train and X validation tensor:', X_train.shape, X_val.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train and X validation tensor: (14752, 1505) (1640, 1505)\n",
            "CPU times: user 217 ms, sys: 16.1 ms, total: 233 ms\n",
            "Wall time: 234 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z24i7TnzlZWC",
        "outputId": "7e483d30-b223-4c1c-f944-a12b74fbb1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "MAX_LEN = X_train.shape[1]\n",
        "gwv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 33.9 s, sys: 4.13 s, total: 38 s\n",
            "Wall time: 1min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMgnDwtZlZZM"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kc8eQ2Yj0Qo",
        "outputId": "55945879-4034-4377-c9be-ab99afa23824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "# Create embedding matrix\n",
        "count_null = 0 \n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = gwv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        count_null += 1\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "print('Count of string that not in word2vec vocabulary:', count_null)\n",
        "print('From %d words' % (vocabulary_size))\n",
        "del(gwv); gc.collect()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of string that not in word2vec vocabulary: 14157\n",
            "From 35462 words\n",
            "CPU times: user 790 ms, sys: 54.9 ms, total: 845 ms\n",
            "Wall time: 844 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i50Oj7iSj0Tf"
      },
      "source": [
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_shape=(MAX_LEN,),\n",
        "                            trainable=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMMgO1vtmEtU"
      },
      "source": [
        "optimizer_adam = Adam(lr = 0.0005)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, restore_best_weights=True)\n",
        "]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM1hALBZj0WS",
        "outputId": "22ee8e98-dcd5-4d23-daa9-732572979a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
        "model.add(Conv1D(32, 11, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_absolute_error', optimizer=optimizer_adam, metrics=['mae', 'mse'])\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1505, 300)         10638600  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 1505, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1505, 64)          64128     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1495, 32)          22560     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 299, 32)           0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial (None, 299, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9568)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9569      \n",
            "=================================================================\n",
            "Total params: 10,734,857\n",
            "Trainable params: 96,257\n",
            "Non-trainable params: 10,638,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5QTc9rnmQ52",
        "outputId": "210b8784-2e26-4748-fab5-06cee6c2eb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# It's just an example of training a model. Real model have another loss\n",
        "model.fit(X_train, np.asarray(y_train), batch_size=512, epochs=120, verbose=1, \n",
        "          callbacks=callbacks, validation_data=(X_val, np.asarray(y_val)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "29/29 [==============================] - 16s 566ms/step - loss: 0.2333 - mae: 0.2333 - mse: 0.0979 - val_loss: 0.2083 - val_mae: 0.2083 - val_mse: 0.1003\n",
            "Epoch 2/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.2088 - mae: 0.2088 - mse: 0.1041 - val_loss: 0.2084 - val_mae: 0.2084 - val_mse: 0.1007\n",
            "Epoch 3/120\n",
            "29/29 [==============================] - 16s 552ms/step - loss: 0.2088 - mae: 0.2088 - mse: 0.1038 - val_loss: 0.2081 - val_mae: 0.2081 - val_mse: 0.0999\n",
            "Epoch 4/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.2079 - mae: 0.2079 - mse: 0.0998 - val_loss: 0.1985 - val_mae: 0.1985 - val_mse: 0.0794\n",
            "Epoch 5/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.1825 - mae: 0.1825 - mse: 0.0585 - val_loss: 0.1526 - val_mae: 0.1526 - val_mse: 0.0406\n",
            "Epoch 6/120\n",
            "29/29 [==============================] - 16s 543ms/step - loss: 0.1456 - mae: 0.1456 - mse: 0.0392 - val_loss: 0.1336 - val_mae: 0.1336 - val_mse: 0.0349\n",
            "Epoch 7/120\n",
            "29/29 [==============================] - 16s 542ms/step - loss: 0.1359 - mae: 0.1359 - mse: 0.0372 - val_loss: 0.1297 - val_mae: 0.1297 - val_mse: 0.0338\n",
            "Epoch 8/120\n",
            "29/29 [==============================] - 16s 542ms/step - loss: 0.1313 - mae: 0.1313 - mse: 0.0345 - val_loss: 0.1284 - val_mae: 0.1284 - val_mse: 0.0321\n",
            "Epoch 9/120\n",
            "29/29 [==============================] - 16s 543ms/step - loss: 0.1292 - mae: 0.1292 - mse: 0.0336 - val_loss: 0.1248 - val_mae: 0.1248 - val_mse: 0.0329\n",
            "Epoch 10/120\n",
            "29/29 [==============================] - 16s 542ms/step - loss: 0.1228 - mae: 0.1228 - mse: 0.0316 - val_loss: 0.1183 - val_mae: 0.1183 - val_mse: 0.0294\n",
            "Epoch 11/120\n",
            "29/29 [==============================] - 16s 544ms/step - loss: 0.1190 - mae: 0.1190 - mse: 0.0296 - val_loss: 0.1158 - val_mae: 0.1158 - val_mse: 0.0280\n",
            "Epoch 12/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.1161 - mae: 0.1161 - mse: 0.0284 - val_loss: 0.1136 - val_mae: 0.1136 - val_mse: 0.0263\n",
            "Epoch 13/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.1129 - mae: 0.1129 - mse: 0.0271 - val_loss: 0.1126 - val_mae: 0.1126 - val_mse: 0.0255\n",
            "Epoch 14/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.1113 - mae: 0.1113 - mse: 0.0265 - val_loss: 0.1109 - val_mae: 0.1109 - val_mse: 0.0253\n",
            "Epoch 15/120\n",
            "29/29 [==============================] - 16s 544ms/step - loss: 0.1094 - mae: 0.1094 - mse: 0.0253 - val_loss: 0.1095 - val_mae: 0.1095 - val_mse: 0.0238\n",
            "Epoch 16/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.1074 - mae: 0.1074 - mse: 0.0247 - val_loss: 0.1074 - val_mae: 0.1074 - val_mse: 0.0244\n",
            "Epoch 17/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.1057 - mae: 0.1057 - mse: 0.0242 - val_loss: 0.1079 - val_mae: 0.1079 - val_mse: 0.0236\n",
            "Epoch 18/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.1047 - mae: 0.1047 - mse: 0.0240 - val_loss: 0.1057 - val_mae: 0.1057 - val_mse: 0.0232\n",
            "Epoch 19/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.1032 - mae: 0.1032 - mse: 0.0234 - val_loss: 0.1055 - val_mae: 0.1055 - val_mse: 0.0242\n",
            "Epoch 20/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.1014 - mae: 0.1014 - mse: 0.0228 - val_loss: 0.1033 - val_mae: 0.1033 - val_mse: 0.0224\n",
            "Epoch 21/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.1008 - mae: 0.1008 - mse: 0.0227 - val_loss: 0.1028 - val_mae: 0.1028 - val_mse: 0.0225\n",
            "Epoch 22/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0987 - mae: 0.0987 - mse: 0.0216 - val_loss: 0.1049 - val_mae: 0.1049 - val_mse: 0.0244\n",
            "Epoch 23/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0987 - mae: 0.0987 - mse: 0.0218 - val_loss: 0.1013 - val_mae: 0.1013 - val_mse: 0.0226\n",
            "Epoch 24/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0978 - mae: 0.0978 - mse: 0.0215 - val_loss: 0.1029 - val_mae: 0.1029 - val_mse: 0.0229\n",
            "Epoch 25/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0971 - mae: 0.0971 - mse: 0.0208 - val_loss: 0.1010 - val_mae: 0.1010 - val_mse: 0.0224\n",
            "Epoch 26/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0957 - mae: 0.0957 - mse: 0.0208 - val_loss: 0.0993 - val_mae: 0.0993 - val_mse: 0.0214\n",
            "Epoch 27/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0945 - mae: 0.0945 - mse: 0.0204 - val_loss: 0.0998 - val_mae: 0.0998 - val_mse: 0.0221\n",
            "Epoch 28/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0935 - mae: 0.0935 - mse: 0.0201 - val_loss: 0.0998 - val_mae: 0.0998 - val_mse: 0.0213\n",
            "Epoch 29/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0932 - mae: 0.0932 - mse: 0.0198 - val_loss: 0.0996 - val_mae: 0.0996 - val_mse: 0.0213\n",
            "Epoch 30/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0932 - mae: 0.0932 - mse: 0.0204 - val_loss: 0.0979 - val_mae: 0.0979 - val_mse: 0.0208\n",
            "Epoch 31/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0935 - mae: 0.0935 - mse: 0.0197 - val_loss: 0.1000 - val_mae: 0.1000 - val_mse: 0.0215\n",
            "Epoch 32/120\n",
            "29/29 [==============================] - 16s 550ms/step - loss: 0.0933 - mae: 0.0933 - mse: 0.0198 - val_loss: 0.0997 - val_mae: 0.0997 - val_mse: 0.0222\n",
            "Epoch 33/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0914 - mae: 0.0914 - mse: 0.0193 - val_loss: 0.0981 - val_mae: 0.0981 - val_mse: 0.0209\n",
            "Epoch 34/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0906 - mae: 0.0906 - mse: 0.0188 - val_loss: 0.0989 - val_mae: 0.0989 - val_mse: 0.0221\n",
            "Epoch 35/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0912 - mae: 0.0912 - mse: 0.0192 - val_loss: 0.0980 - val_mae: 0.0980 - val_mse: 0.0216\n",
            "Epoch 36/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.0900 - mae: 0.0900 - mse: 0.0187 - val_loss: 0.0979 - val_mae: 0.0979 - val_mse: 0.0214\n",
            "Epoch 37/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0882 - mae: 0.0882 - mse: 0.0180 - val_loss: 0.0997 - val_mae: 0.0997 - val_mse: 0.0213\n",
            "Epoch 38/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0891 - mae: 0.0891 - mse: 0.0185 - val_loss: 0.0981 - val_mae: 0.0981 - val_mse: 0.0207\n",
            "Epoch 39/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0875 - mae: 0.0875 - mse: 0.0176 - val_loss: 0.0974 - val_mae: 0.0974 - val_mse: 0.0212\n",
            "Epoch 40/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0867 - mae: 0.0867 - mse: 0.0174 - val_loss: 0.0967 - val_mae: 0.0967 - val_mse: 0.0211\n",
            "Epoch 41/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0866 - mae: 0.0866 - mse: 0.0174 - val_loss: 0.0962 - val_mae: 0.0962 - val_mse: 0.0205\n",
            "Epoch 42/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0877 - mae: 0.0877 - mse: 0.0179 - val_loss: 0.0975 - val_mae: 0.0975 - val_mse: 0.0216\n",
            "Epoch 43/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0850 - mae: 0.0850 - mse: 0.0171 - val_loss: 0.0975 - val_mae: 0.0975 - val_mse: 0.0206\n",
            "Epoch 44/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0848 - mae: 0.0848 - mse: 0.0168 - val_loss: 0.0964 - val_mae: 0.0964 - val_mse: 0.0213\n",
            "Epoch 45/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0858 - mae: 0.0858 - mse: 0.0172 - val_loss: 0.0962 - val_mae: 0.0962 - val_mse: 0.0212\n",
            "Epoch 46/120\n",
            "29/29 [==============================] - 16s 548ms/step - loss: 0.0842 - mae: 0.0842 - mse: 0.0167 - val_loss: 0.0966 - val_mae: 0.0966 - val_mse: 0.0214\n",
            "Epoch 47/120\n",
            "29/29 [==============================] - 16s 550ms/step - loss: 0.0842 - mae: 0.0842 - mse: 0.0167 - val_loss: 0.0955 - val_mae: 0.0955 - val_mse: 0.0201\n",
            "Epoch 48/120\n",
            "29/29 [==============================] - 16s 551ms/step - loss: 0.0834 - mae: 0.0834 - mse: 0.0164 - val_loss: 0.0958 - val_mae: 0.0958 - val_mse: 0.0207\n",
            "Epoch 49/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0831 - mae: 0.0831 - mse: 0.0162 - val_loss: 0.0960 - val_mae: 0.0960 - val_mse: 0.0207\n",
            "Epoch 50/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0815 - mae: 0.0815 - mse: 0.0159 - val_loss: 0.0958 - val_mae: 0.0958 - val_mse: 0.0204\n",
            "Epoch 51/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0815 - mae: 0.0815 - mse: 0.0157 - val_loss: 0.0962 - val_mae: 0.0962 - val_mse: 0.0210\n",
            "Epoch 52/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.0801 - mae: 0.0801 - mse: 0.0153 - val_loss: 0.0962 - val_mae: 0.0962 - val_mse: 0.0206\n",
            "Epoch 53/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0811 - mae: 0.0811 - mse: 0.0156 - val_loss: 0.0951 - val_mae: 0.0951 - val_mse: 0.0206\n",
            "Epoch 54/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0797 - mae: 0.0797 - mse: 0.0153 - val_loss: 0.0976 - val_mae: 0.0976 - val_mse: 0.0209\n",
            "Epoch 55/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0800 - mae: 0.0800 - mse: 0.0151 - val_loss: 0.0958 - val_mae: 0.0958 - val_mse: 0.0209\n",
            "Epoch 56/120\n",
            "29/29 [==============================] - 16s 546ms/step - loss: 0.0796 - mae: 0.0796 - mse: 0.0150 - val_loss: 0.0955 - val_mae: 0.0955 - val_mse: 0.0213\n",
            "Epoch 57/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0783 - mae: 0.0783 - mse: 0.0147 - val_loss: 0.0953 - val_mae: 0.0953 - val_mse: 0.0208\n",
            "Epoch 58/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0785 - mae: 0.0785 - mse: 0.0147 - val_loss: 0.0958 - val_mae: 0.0958 - val_mse: 0.0211\n",
            "Epoch 59/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0779 - mae: 0.0779 - mse: 0.0146 - val_loss: 0.0971 - val_mae: 0.0971 - val_mse: 0.0221\n",
            "Epoch 60/120\n",
            "29/29 [==============================] - 16s 545ms/step - loss: 0.0770 - mae: 0.0770 - mse: 0.0143 - val_loss: 0.1008 - val_mae: 0.1008 - val_mse: 0.0216\n",
            "Epoch 61/120\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.0775 - mae: 0.0775 - mse: 0.0143 - val_loss: 0.0957 - val_mae: 0.0957 - val_mse: 0.0213\n",
            "Epoch 62/120\n",
            "29/29 [==============================] - 16s 549ms/step - loss: 0.0770 - mae: 0.0770 - mse: 0.0144 - val_loss: 0.0975 - val_mae: 0.0975 - val_mse: 0.0218\n",
            "Epoch 63/120\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.0756 - mae: 0.0756 - mse: 0.0138Restoring model weights from the end of the best epoch.\n",
            "29/29 [==============================] - 16s 551ms/step - loss: 0.0756 - mae: 0.0756 - mse: 0.0138 - val_loss: 0.0965 - val_mae: 0.0965 - val_mse: 0.0204\n",
            "Epoch 00063: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f919c5c1e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NqPxOBCmQ9G",
        "outputId": "7cd07537-5dee-4129-d7ad-213de4a70337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "test=pd.read_csv('test.csv')\n",
        "test.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ehhh better punta cana twice compared hotel st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4 n't think, decided book atenea night stay de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>awesome time just returned vacation fantastic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>grand oasis wonderful second time, group 20 fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>not bad stay stayed hotel family attending jav...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                             review\n",
              "0   0  ehhh better punta cana twice compared hotel st...\n",
              "1   1  4 n't think, decided book atenea night stay de...\n",
              "2   2  awesome time just returned vacation fantastic ...\n",
              "3   3  grand oasis wonderful second time, group 20 fr...\n",
              "4   4  not bad stay stayed hotel family attending jav..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXcJlnz2mQ_6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from scripts.review_scoring import ReviewScorer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmb6x4dsnJSL"
      },
      "source": [
        "!cp *.pkl /content/models/"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IypAwZiyvm8Z",
        "outputId": "216f645d-e8b8-43ad-ab4d-3701fd6502c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "scorer = ReviewScorer('models/model.h5', 'models/my_tokenizer.pkl')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model was loaded from models/my_tokenizer.pkl\n",
            "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5F_L7xnJXU"
      },
      "source": [
        "p=scorer.predict(['It\\'s the best hotel I ever see'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMB-TblB0hav",
        "outputId": "e868d522-1a43-4550-dad0-9af2788a059b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8338482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buK3exQdnJbS",
        "outputId": "d24683be-74f6-4136-e6b0-fd11b15a5fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test['rating']=0\n",
        "for i in test.index:\n",
        "  text=test['review'][i]\n",
        "  score=scorer.predict([text])\n",
        "  s=int(round(score[0]*5))\n",
        "  test['rating'][i]=s\n",
        "  print(i,s)\n",
        "  \n",
        "  #if i==10:\n",
        "  #  break\n",
        "\n",
        "test.to_csv(\"rating_sub.csv\",columns=['id','rating'],header=None,index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3\n",
            "1 3\n",
            "2 4\n",
            "3 4\n",
            "4 4\n",
            "5 5\n",
            "6 4\n",
            "7 4\n",
            "8 5\n",
            "9 2\n",
            "10 4\n",
            "11 4\n",
            "12 4\n",
            "13 4\n",
            "14 3\n",
            "15 4\n",
            "16 4\n",
            "17 3\n",
            "18 4\n",
            "19 4\n",
            "20 4\n",
            "21 3\n",
            "22 4\n",
            "23 4\n",
            "24 4\n",
            "25 4\n",
            "26 4\n",
            "27 4\n",
            "28 5\n",
            "29 4\n",
            "30 4\n",
            "31 3\n",
            "32 4\n",
            "33 4\n",
            "34 4\n",
            "35 3\n",
            "36 4\n",
            "37 3\n",
            "38 4\n",
            "39 3\n",
            "40 4\n",
            "41 4\n",
            "42 5\n",
            "43 4\n",
            "44 4\n",
            "45 4\n",
            "46 5\n",
            "47 4\n",
            "48 5\n",
            "49 4\n",
            "50 5\n",
            "51 4\n",
            "52 4\n",
            "53 3\n",
            "54 4\n",
            "55 4\n",
            "56 3\n",
            "57 4\n",
            "58 4\n",
            "59 3\n",
            "60 4\n",
            "61 3\n",
            "62 3\n",
            "63 4\n",
            "64 5\n",
            "65 4\n",
            "66 5\n",
            "67 4\n",
            "68 4\n",
            "69 4\n",
            "70 5\n",
            "71 5\n",
            "72 3\n",
            "73 3\n",
            "74 4\n",
            "75 4\n",
            "76 4\n",
            "77 4\n",
            "78 4\n",
            "79 4\n",
            "80 5\n",
            "81 4\n",
            "82 3\n",
            "83 4\n",
            "84 4\n",
            "85 4\n",
            "86 5\n",
            "87 3\n",
            "88 4\n",
            "89 4\n",
            "90 5\n",
            "91 4\n",
            "92 5\n",
            "93 4\n",
            "94 5\n",
            "95 3\n",
            "96 5\n",
            "97 4\n",
            "98 4\n",
            "99 4\n",
            "100 4\n",
            "101 5\n",
            "102 4\n",
            "103 4\n",
            "104 5\n",
            "105 4\n",
            "106 4\n",
            "107 5\n",
            "108 5\n",
            "109 3\n",
            "110 4\n",
            "111 4\n",
            "112 4\n",
            "113 4\n",
            "114 4\n",
            "115 3\n",
            "116 5\n",
            "117 4\n",
            "118 5\n",
            "119 3\n",
            "120 3\n",
            "121 3\n",
            "122 4\n",
            "123 3\n",
            "124 5\n",
            "125 4\n",
            "126 5\n",
            "127 4\n",
            "128 4\n",
            "129 4\n",
            "130 3\n",
            "131 4\n",
            "132 3\n",
            "133 4\n",
            "134 4\n",
            "135 3\n",
            "136 4\n",
            "137 4\n",
            "138 4\n",
            "139 3\n",
            "140 4\n",
            "141 5\n",
            "142 3\n",
            "143 4\n",
            "144 4\n",
            "145 4\n",
            "146 3\n",
            "147 5\n",
            "148 4\n",
            "149 4\n",
            "150 5\n",
            "151 5\n",
            "152 4\n",
            "153 4\n",
            "154 3\n",
            "155 4\n",
            "156 3\n",
            "157 4\n",
            "158 5\n",
            "159 5\n",
            "160 3\n",
            "161 4\n",
            "162 4\n",
            "163 5\n",
            "164 4\n",
            "165 4\n",
            "166 4\n",
            "167 4\n",
            "168 5\n",
            "169 3\n",
            "170 5\n",
            "171 4\n",
            "172 4\n",
            "173 5\n",
            "174 3\n",
            "175 4\n",
            "176 4\n",
            "177 4\n",
            "178 3\n",
            "179 4\n",
            "180 4\n",
            "181 3\n",
            "182 3\n",
            "183 3\n",
            "184 3\n",
            "185 4\n",
            "186 4\n",
            "187 4\n",
            "188 4\n",
            "189 4\n",
            "190 3\n",
            "191 5\n",
            "192 4\n",
            "193 4\n",
            "194 4\n",
            "195 4\n",
            "196 4\n",
            "197 4\n",
            "198 4\n",
            "199 4\n",
            "200 4\n",
            "201 5\n",
            "202 4\n",
            "203 3\n",
            "204 4\n",
            "205 4\n",
            "206 4\n",
            "207 4\n",
            "208 3\n",
            "209 4\n",
            "210 3\n",
            "211 4\n",
            "212 5\n",
            "213 4\n",
            "214 4\n",
            "215 4\n",
            "216 4\n",
            "217 3\n",
            "218 4\n",
            "219 4\n",
            "220 5\n",
            "221 4\n",
            "222 4\n",
            "223 3\n",
            "224 5\n",
            "225 4\n",
            "226 5\n",
            "227 2\n",
            "228 4\n",
            "229 4\n",
            "230 5\n",
            "231 4\n",
            "232 4\n",
            "233 4\n",
            "234 4\n",
            "235 4\n",
            "236 4\n",
            "237 4\n",
            "238 3\n",
            "239 4\n",
            "240 4\n",
            "241 4\n",
            "242 4\n",
            "243 4\n",
            "244 4\n",
            "245 4\n",
            "246 4\n",
            "247 5\n",
            "248 4\n",
            "249 3\n",
            "250 5\n",
            "251 3\n",
            "252 3\n",
            "253 3\n",
            "254 4\n",
            "255 3\n",
            "256 4\n",
            "257 4\n",
            "258 4\n",
            "259 4\n",
            "260 3\n",
            "261 3\n",
            "262 4\n",
            "263 5\n",
            "264 5\n",
            "265 4\n",
            "266 5\n",
            "267 3\n",
            "268 4\n",
            "269 5\n",
            "270 4\n",
            "271 4\n",
            "272 3\n",
            "273 5\n",
            "274 4\n",
            "275 3\n",
            "276 4\n",
            "277 4\n",
            "278 4\n",
            "279 5\n",
            "280 4\n",
            "281 4\n",
            "282 5\n",
            "283 4\n",
            "284 4\n",
            "285 5\n",
            "286 4\n",
            "287 4\n",
            "288 4\n",
            "289 4\n",
            "290 4\n",
            "291 5\n",
            "292 4\n",
            "293 4\n",
            "294 4\n",
            "295 3\n",
            "296 4\n",
            "297 3\n",
            "298 4\n",
            "299 3\n",
            "300 4\n",
            "301 4\n",
            "302 4\n",
            "303 4\n",
            "304 4\n",
            "305 3\n",
            "306 5\n",
            "307 4\n",
            "308 4\n",
            "309 5\n",
            "310 3\n",
            "311 5\n",
            "312 4\n",
            "313 4\n",
            "314 4\n",
            "315 4\n",
            "316 4\n",
            "317 4\n",
            "318 5\n",
            "319 5\n",
            "320 4\n",
            "321 4\n",
            "322 4\n",
            "323 4\n",
            "324 3\n",
            "325 5\n",
            "326 3\n",
            "327 4\n",
            "328 4\n",
            "329 4\n",
            "330 4\n",
            "331 4\n",
            "332 3\n",
            "333 4\n",
            "334 4\n",
            "335 4\n",
            "336 3\n",
            "337 4\n",
            "338 3\n",
            "339 3\n",
            "340 3\n",
            "341 4\n",
            "342 4\n",
            "343 4\n",
            "344 4\n",
            "345 5\n",
            "346 4\n",
            "347 4\n",
            "348 5\n",
            "349 4\n",
            "350 3\n",
            "351 4\n",
            "352 4\n",
            "353 4\n",
            "354 3\n",
            "355 4\n",
            "356 3\n",
            "357 4\n",
            "358 4\n",
            "359 5\n",
            "360 3\n",
            "361 4\n",
            "362 5\n",
            "363 3\n",
            "364 4\n",
            "365 4\n",
            "366 4\n",
            "367 3\n",
            "368 4\n",
            "369 4\n",
            "370 3\n",
            "371 4\n",
            "372 5\n",
            "373 4\n",
            "374 3\n",
            "375 4\n",
            "376 3\n",
            "377 4\n",
            "378 3\n",
            "379 4\n",
            "380 4\n",
            "381 5\n",
            "382 5\n",
            "383 4\n",
            "384 4\n",
            "385 4\n",
            "386 2\n",
            "387 5\n",
            "388 5\n",
            "389 4\n",
            "390 3\n",
            "391 4\n",
            "392 3\n",
            "393 4\n",
            "394 4\n",
            "395 4\n",
            "396 4\n",
            "397 4\n",
            "398 4\n",
            "399 4\n",
            "400 3\n",
            "401 4\n",
            "402 5\n",
            "403 4\n",
            "404 4\n",
            "405 4\n",
            "406 4\n",
            "407 3\n",
            "408 4\n",
            "409 4\n",
            "410 4\n",
            "411 5\n",
            "412 3\n",
            "413 3\n",
            "414 4\n",
            "415 5\n",
            "416 4\n",
            "417 3\n",
            "418 4\n",
            "419 4\n",
            "420 5\n",
            "421 4\n",
            "422 3\n",
            "423 4\n",
            "424 3\n",
            "425 4\n",
            "426 4\n",
            "427 4\n",
            "428 5\n",
            "429 3\n",
            "430 3\n",
            "431 4\n",
            "432 3\n",
            "433 4\n",
            "434 5\n",
            "435 4\n",
            "436 4\n",
            "437 4\n",
            "438 4\n",
            "439 4\n",
            "440 3\n",
            "441 4\n",
            "442 3\n",
            "443 4\n",
            "444 4\n",
            "445 5\n",
            "446 3\n",
            "447 5\n",
            "448 4\n",
            "449 4\n",
            "450 4\n",
            "451 3\n",
            "452 4\n",
            "453 4\n",
            "454 3\n",
            "455 4\n",
            "456 4\n",
            "457 3\n",
            "458 4\n",
            "459 4\n",
            "460 4\n",
            "461 4\n",
            "462 4\n",
            "463 3\n",
            "464 3\n",
            "465 4\n",
            "466 5\n",
            "467 4\n",
            "468 3\n",
            "469 4\n",
            "470 4\n",
            "471 4\n",
            "472 3\n",
            "473 4\n",
            "474 3\n",
            "475 4\n",
            "476 4\n",
            "477 4\n",
            "478 3\n",
            "479 4\n",
            "480 4\n",
            "481 4\n",
            "482 4\n",
            "483 3\n",
            "484 4\n",
            "485 4\n",
            "486 4\n",
            "487 3\n",
            "488 5\n",
            "489 3\n",
            "490 5\n",
            "491 3\n",
            "492 4\n",
            "493 3\n",
            "494 3\n",
            "495 3\n",
            "496 4\n",
            "497 4\n",
            "498 3\n",
            "499 4\n",
            "500 4\n",
            "501 3\n",
            "502 3\n",
            "503 5\n",
            "504 3\n",
            "505 4\n",
            "506 4\n",
            "507 3\n",
            "508 4\n",
            "509 4\n",
            "510 5\n",
            "511 4\n",
            "512 4\n",
            "513 3\n",
            "514 3\n",
            "515 4\n",
            "516 4\n",
            "517 4\n",
            "518 5\n",
            "519 4\n",
            "520 3\n",
            "521 4\n",
            "522 4\n",
            "523 4\n",
            "524 4\n",
            "525 3\n",
            "526 4\n",
            "527 3\n",
            "528 4\n",
            "529 4\n",
            "530 4\n",
            "531 3\n",
            "532 3\n",
            "533 4\n",
            "534 3\n",
            "535 5\n",
            "536 4\n",
            "537 3\n",
            "538 4\n",
            "539 4\n",
            "540 4\n",
            "541 4\n",
            "542 4\n",
            "543 4\n",
            "544 5\n",
            "545 5\n",
            "546 4\n",
            "547 4\n",
            "548 5\n",
            "549 4\n",
            "550 3\n",
            "551 4\n",
            "552 3\n",
            "553 4\n",
            "554 3\n",
            "555 5\n",
            "556 3\n",
            "557 4\n",
            "558 4\n",
            "559 4\n",
            "560 5\n",
            "561 4\n",
            "562 5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}